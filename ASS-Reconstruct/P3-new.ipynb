{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "from enum import Enum\n",
    "class ACTION(Enum):\n",
    "    low=0 \n",
    "    high=1\n",
    "class Winner:\n",
    "    def __init__(self):\n",
    "        self.energy_max = 10\n",
    "        self.energy_add = 2\n",
    "        self.energy_prob = 0.2\n",
    "        self.P_H = 0.55\n",
    "        self.P_L = 0.45\n",
    "        self.state = 0\n",
    "        self.done = 0\n",
    "        self.R = 0\n",
    "        self.highcost = 1\n",
    "        self.lowcost = 0\n",
    "        self.greedy = 0.1\n",
    "        self.energy=self.energy_max\n",
    "        self.discount=0.9\n",
    "        self.alpha=0.01\n",
    "        self.lam = 0.9\n",
    "        return\n",
    "    def reset_game(self):\n",
    "        self.state=0\n",
    "        self.done=0\n",
    "        self.energy=self.energy_max\n",
    "        return\n",
    "\n",
    "        \n",
    "        \n",
    "    def env_step(self,state,energy,action):\n",
    "        done=0\n",
    "        reward=0\n",
    "        sample = np.random.uniform(0, 1)\n",
    "        smp = np.random.uniform(0,1)\n",
    "\n",
    "        if action == ACTION.high:\n",
    "            if energy >= self.highcost :\n",
    "                suc_prob = self.P_H\n",
    "                energy = energy - self.highcost\n",
    "            else:\n",
    "                suc_prob = self.P_L\n",
    "                reward = reward + self.lowcost\n",
    "        else: \n",
    "            suc_prob = self.P_L\n",
    "            reward = reward + self.lowcost\n",
    "        if sample < suc_prob:\n",
    "            state = state + 1\n",
    "        else:\n",
    "            state = state -1\n",
    "        if state >= 3 :\n",
    "            reward = reward + 1000\n",
    "            done = 1\n",
    "        if state <= (-3):\n",
    "            done = 1\n",
    "        if smp < self.energy_prob:\n",
    "            energy = energy + self.energy_add\n",
    "        if energy > self.energy_max:\n",
    "            energy = self.energy_max\n",
    "        return reward,state,energy,done\n",
    "    def ba_policy(self,state,B):\n",
    "        \n",
    "        sample = np.random.uniform(0, 1)\n",
    "        \n",
    "        loweffort = self.fea_vector(state,B,ACTION.low)\n",
    "        \n",
    "        higheffort = self.fea_vector(state,B,ACTION.high)\n",
    "        \n",
    "        Q_low=np.dot(self.w,loweffort)\n",
    "        \n",
    "        Q_high=np.dot(self.w,higheffort)\n",
    "        \n",
    "        if sample > self.greedy: \n",
    "        \n",
    "            act_buf = np.argmax([Q_low,Q_high])\n",
    "            if act_buf == 1:\n",
    "                act = ACTION.high\n",
    "            else:\n",
    "                act = ACTION.low\n",
    "            #print(act)\n",
    "        else:\n",
    "            \n",
    "            smp = np.random.uniform(0,1)\n",
    "            \n",
    "            if smp > 0.5:\n",
    "                act = ACTION.high\n",
    "            else:\n",
    "                act = ACTION.low\n",
    "        #print(act)\n",
    "        return act\n",
    "\n",
    "    def SARSAlambda(self,num_episodes):\n",
    "        \n",
    "        self.w=self.init_weight()\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            B = self.energy_max\n",
    "            done = 0\n",
    "\n",
    "            state = 0\n",
    "            q_old = 0\n",
    "            z= 0\n",
    "            action = self.ba_policy(state,B)\n",
    "            #print(action)\n",
    "            x = self.fea_vector(state,B,action)\n",
    "            while(1):\n",
    "                \n",
    "                reward,next_state,B,done = self.env_step(state,B,action)\n",
    "                #print('flag=',flag,next)\n",
    "                next_action = self.ba_policy(next_state,B)\n",
    "                \n",
    "                next_x = self.fea_vector(next_state,B,next_action)\n",
    "                \n",
    "                q = np.dot(self.w,x)\n",
    "                \n",
    "                next_q = np.dot(self.w,next_x)\n",
    "                \n",
    "                delta = reward + self.discount*next_q - q\n",
    "                \n",
    "                z = self.discount*self.lam*z + (1 - self.alpha*self.discount*self.lam*np.dot(z,x))*x\n",
    "                \n",
    "                self.w = self.w + self.alpha*(delta+ q - q_old)*z - self.alpha * (q - q_old)*x\n",
    "                \n",
    "                q_old = next_q\n",
    "                \n",
    "                x = next_x\n",
    "                \n",
    "                action = next_action\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done == 1:\n",
    "                    break\n",
    "\n",
    "        return self.w\n",
    "    def init_weight(self):\n",
    "        w = np.zeros((154))\n",
    "        return w\n",
    "    def fea_vector(self,state,B,action):\n",
    "        x = np.zeros((154))\n",
    "        #print(state)\n",
    "        x[action.value*76+(state+3)*11+B] = 1\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "winner=Winner()\n",
    "\n",
    "Q=winner.SARSAlambda(500000)\n",
    "\n",
    "Q = defaultdict(lambda:np.zeros(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f902eef4bf8>, {(-3, 0): array([0., 0.]), (-3, 1): array([0., 0.]), (-3, 2): array([0., 0.]), (-3, 3): array([0., 0.]), (-3, 4): array([0., 0.]), (-3, 5): array([0., 0.]), (-3, 6): array([0., 0.]), (-3, 7): array([0., 0.]), (-3, 8): array([0., 0.]), (-3, 9): array([0., 0.]), (-3, 10): array([0., 0.]), (-2, 0): array([1.76810086, 0.0021099 ]), (-2, 1): array([0.90068246, 0.        ]), (-2, 2): array([28.53389773,  2.37244803]), (-2, 3): array([57.28657471, 18.67765224]), (-2, 4): array([47.6822711 , 58.64128081]), (-2, 5): array([57.01914762, 43.12527567]), (-2, 6): array([62.24174462, 73.17138695]), (-2, 7): array([23.572231  , 71.67051977]), (-2, 8): array([75.63968915, 88.5933672 ]), (-2, 9): array([69.34510218, 62.59466434]), (-2, 10): array([66.13227584, 93.07725228]), (-1, 0): array([1.21144705e+01, 1.95959294e-03]), (-1, 1): array([0.61270715, 7.32699072]), (-1, 2): array([85.08331213,  7.73483186]), (-1, 3): array([131.90754813,  79.98059446]), (-1, 4): array([123.63284731, 106.55519379]), (-1, 5): array([133.02347949, 149.26390646]), (-1, 6): array([151.43470128, 124.95961903]), (-1, 7): array([147.6285717 , 182.72309403]), (-1, 8): array([174.181887  , 145.99330574]), (-1, 9): array([153.27357175, 193.55442173]), (-1, 10): array([163.54351193, 175.95662528]), (0, 0): array([ 0.32263705, 22.56445017]), (0, 1): array([ 1.99793686, 19.35370459]), (0, 2): array([170.74779669,  22.15293386]), (0, 3): array([229.63998668, 125.44602606]), (0, 4): array([227.82258618, 247.24759413]), (0, 5): array([254.96746085, 207.9744125 ]), (0, 6): array([246.03732099, 297.29161008]), (0, 7): array([280.35590437, 182.383038  ]), (0, 8): array([272.04777786, 324.52176612]), (0, 9): array([278.44703937, 298.34225086]), (0, 10): array([267.02927508, 319.77585624]), (1, 0): array([36.92842838,  2.30104846]), (1, 1): array([32.44088288,  0.81696403]), (1, 2): array([289.70796849,  36.78175681]), (1, 3): array([366.04991175, 267.21518674]), (1, 4): array([406.71186264, 336.27893433]), (1, 5): array([391.58995657, 459.77201305]), (1, 6): array([431.58656717, 380.73743655]), (1, 7): array([421.68948401, 485.12252501]), (1, 8): array([455.48474257, 399.07532235]), (1, 9): array([426.70812928, 499.90701577]), (1, 10): array([455.95276805, 469.6949635 ]), (2, 0): array([28.91413963,  0.08710043]), (2, 1): array([46.10474964,  3.00897459]), (2, 2): array([439.8408477 ,  34.81775176]), (2, 3): array([630.76384108, 257.23414594]), (2, 4): array([643.37159122, 688.64637318]), (2, 5): array([672.92957696, 485.48851824]), (2, 6): array([660.28567703, 735.39098018]), (2, 7): array([687.89311151, 296.48641183]), (2, 8): array([678.55434544, 735.89785112]), (2, 9): array([687.02031102, 683.00651072]), (2, 10): array([681.78160003, 747.61045189]), (3, 0): array([0., 0.]), (3, 1): array([0., 0.]), (3, 2): array([0., 0.]), (3, 3): array([0., 0.]), (3, 4): array([0., 0.]), (3, 5): array([0., 0.]), (3, 6): array([0., 0.]), (3, 7): array([0., 0.]), (3, 8): array([0., 0.]), (3, 9): array([0., 0.]), (3, 10): array([0., 0.])})\n"
     ]
    }
   ],
   "source": [
    "B_MAX=10\n",
    "for state in range (7):\n",
    "    for b in range(B_MAX+1):\n",
    "        Q[state-3,b][0]=np.dot(winner.w,winner.fea_vector(state - 3,b,ACTION.low))\n",
    "        Q[state-3,b][1]=np.dot(winner.w,winner.fea_vector(state - 3,b,ACTION.high))\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              low        high\n",
      "-3 0     0.000000    0.000000\n",
      "   1     0.000000    0.000000\n",
      "   2     0.000000    0.000000\n",
      "   3     0.000000    0.000000\n",
      "   4     0.000000    0.000000\n",
      "   5     0.000000    0.000000\n",
      "   6     0.000000    0.000000\n",
      "   7     0.000000    0.000000\n",
      "   8     0.000000    0.000000\n",
      "   9     0.000000    0.000000\n",
      "   10    0.000000    0.000000\n",
      "-2 0     1.768101    0.002110\n",
      "   1     0.900682    0.000000\n",
      "   2    28.533898    2.372448\n",
      "   3    57.286575   18.677652\n",
      "   4    47.682271   58.641281\n",
      "   5    57.019148   43.125276\n",
      "   6    62.241745   73.171387\n",
      "   7    23.572231   71.670520\n",
      "   8    75.639689   88.593367\n",
      "   9    69.345102   62.594664\n",
      "   10   66.132276   93.077252\n",
      "-1 0    12.114471    0.001960\n",
      "   1     0.612707    7.326991\n",
      "   2    85.083312    7.734832\n",
      "   3   131.907548   79.980594\n",
      "   4   123.632847  106.555194\n",
      "   5   133.023479  149.263906\n",
      "   6   151.434701  124.959619\n",
      "   7   147.628572  182.723094\n",
      "...           ...         ...\n",
      " 1 3   366.049912  267.215187\n",
      "   4   406.711863  336.278934\n",
      "   5   391.589957  459.772013\n",
      "   6   431.586567  380.737437\n",
      "   7   421.689484  485.122525\n",
      "   8   455.484743  399.075322\n",
      "   9   426.708129  499.907016\n",
      "   10  455.952768  469.694964\n",
      " 2 0    28.914140    0.087100\n",
      "   1    46.104750    3.008975\n",
      "   2   439.840848   34.817752\n",
      "   3   630.763841  257.234146\n",
      "   4   643.371591  688.646373\n",
      "   5   672.929577  485.488518\n",
      "   6   660.285677  735.390980\n",
      "   7   687.893112  296.486412\n",
      "   8   678.554345  735.897851\n",
      "   9   687.020311  683.006511\n",
      "   10  681.781600  747.610452\n",
      " 3 0     0.000000    0.000000\n",
      "   1     0.000000    0.000000\n",
      "   2     0.000000    0.000000\n",
      "   3     0.000000    0.000000\n",
      "   4     0.000000    0.000000\n",
      "   5     0.000000    0.000000\n",
      "   6     0.000000    0.000000\n",
      "   7     0.000000    0.000000\n",
      "   8     0.000000    0.000000\n",
      "   9     0.000000    0.000000\n",
      "   10    0.000000    0.000000\n",
      "\n",
      "[77 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "actions={'low','high'}\n",
    "q_table = pd.DataFrame(data=[[Q[state,B][act.value] for act in ACTION] for state,B in Q.keys()],\n",
    "                       index=Q.keys(), columns=actions)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  13.6526239 ,  63.2013982 , 180.49335898,\n",
       "       352.47266825, 659.90186272,   0.        ,  24.00761432,\n",
       "        60.857246  ,  76.00268915, 105.23112663, 122.58073925,\n",
       "       133.70445774, 142.94243696, 151.12219688, 166.02622473,\n",
       "       168.61767823, 118.62950216,  62.49636359])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
