{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "from enum import Enum\n",
    "class ACTION(Enum):\n",
    "    high='high'\n",
    "    low='low' \n",
    "    \n",
    "class Winner:\n",
    "    def __init__(self):\n",
    "        self.P_H = 0.55\n",
    "        self.P_L = 0.45\n",
    "        self.state = 0\n",
    "        self.done = 0\n",
    "        self.R = 0\n",
    "        self.th=0.000001\n",
    "        self.highcost = -50\n",
    "        self.lowcost = -10\n",
    "        self.greedy = 0.1\n",
    "        self.discount=1.0\n",
    "        self.init_Q()\n",
    "        self.init_C()\n",
    "    def reset_game(self):\n",
    "        self.state=0\n",
    "        self.done=0\n",
    "    def ba_policy(self):\n",
    "        smp = np.random.uniform(0,1)\n",
    "        if smp <= 0.5:\n",
    "            ac = ACTION.high\n",
    "        else:\n",
    "            ac = ACTION.low\n",
    "        return ac\n",
    "        \n",
    "    #def ta_policy(self):\n",
    "    def OffpolicyMCES(self,numepisode):\n",
    "        self.init_Q()\n",
    "        self.init_C()\n",
    "        for i_episode in range(1, numepisode+1):\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, numepisode), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            self.reset_game()\n",
    "            state=0\n",
    "            episode = []\n",
    "            while(1):\n",
    "                action = self.ba_policy()\n",
    "                reward,next_state,done = self.env_step(state,action)\n",
    "                episode.append((state, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            G = 0.0\n",
    "            Weight = 1.0\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G = self.discount * G + reward\n",
    "                self.C[state,action] += Weight\n",
    "                self.Q[state,action] += (Weight / self.C[state,action]) * (G - self.Q[state,action])\n",
    "                if state > 1:\n",
    "                    if action == ACTION.low:\n",
    "                        Weight = Weight * (2.0)\n",
    "                    else:\n",
    "                        Weight=0\n",
    "                        break\n",
    "                else:\n",
    "                    if action == ACTION.high:\n",
    "                        Weight = Weight*(2.0)\n",
    "                    else:\n",
    "                        Weight=0\n",
    "                        break\n",
    "            \n",
    "           \n",
    "        return self.Q\n",
    "        \n",
    "        \n",
    "    def env_step(self,state,action):\n",
    "        done=0\n",
    "        reward=0\n",
    "        sample = np.random.uniform(0, 1)\n",
    "        if action == ACTION.high:\n",
    "            reward = reward + self.highcost\n",
    "            suc_prob = self.P_H\n",
    "        else:\n",
    "            suc_prob = self.P_L\n",
    "            reward = reward + self.lowcost\n",
    "        if sample < suc_prob:\n",
    "            state = state + 1\n",
    "        else:\n",
    "            state = state -1\n",
    "        if state >= 3 :\n",
    "            reward = reward + 1000\n",
    "            done = 1\n",
    "        if state <= (-3):\n",
    "            done = 1\n",
    "        return reward,state,done\n",
    "    def init_Q(self):\n",
    "        self.Q = defaultdict(lambda:np.zeros(1))\n",
    "        for state in range(7):\n",
    "            for act in ACTION:\n",
    "                    self.Q[state-3,act]=0.0\n",
    "    def init_C(self):\n",
    "        \n",
    "        self.C = defaultdict(lambda:np.zeros(1))\n",
    "        for state in range(7):\n",
    "            for act in ACTION:\n",
    "                    self.C[state-3,act]=0.0\n",
    "    def reward_expection(self,act,state,V):\n",
    "        #global ph,ch,pl,cl\n",
    "        if act == ACTION.high:\n",
    "            #print('nopro')\n",
    "            if state == 2:\n",
    "                expected_reward = self.P_H*(1000+self.highcost+self.discount*V[state+1]) + self.P_L*(self.highcost+self.discount*V[state-1])\n",
    "            else:\n",
    "                expected_reward = self.P_H*(self.highcost+self.discount*V[state+1]) + self.P_L*(self.highcost+self.discount*V[state-1])\n",
    "                #print('exp',expected_reward)\n",
    "        elif act == ACTION.low:\n",
    "            if state == 2:\n",
    "                expected_reward = self.P_L*(1000+self.lowcost+self.discount*V[state+1]) + self.P_H*(self.lowcost+self.discount*V[state-1])\n",
    "            else:\n",
    "                expected_reward = self.P_L*(self.lowcost+self.discount*V[state+1]) + self.P_H*(self.lowcost+self.discount*V[state-1])\n",
    "       \n",
    "        return expected_reward\n",
    "    \n",
    "    def initV(self,V):\n",
    "        for state in range(7):\n",
    "            V[state-3] = 0.0\n",
    "        return V\n",
    "    def P_E(self,policy):\n",
    "        V = defaultdict(lambda: np.zeros(0))\n",
    "        \n",
    "        \n",
    "        \n",
    "        V = self.initV(V)\n",
    "        \n",
    "        \n",
    "        while(1):\n",
    "            delta = 0\n",
    "            for state in range (5):\n",
    "                state = state - 2\n",
    "                v_buf = V[state]\n",
    "                #print('policy',policy[state])\n",
    "                V[state] = self.reward_expection(act=policy[state],state=state,V=V)\n",
    "                #print('Vs',V[state])\n",
    "                delta = max(delta,abs(v_buf - V[state]))\n",
    "                #print('delta',delta)\n",
    "            if delta < self.th:\n",
    "                break\n",
    "\n",
    "        return V,policy\n",
    "    def cr_pl(self):\n",
    "        pll = []\n",
    "        \n",
    "        for code in range(32):\n",
    "            buf = defaultdict(lambda: np.zeros(0))\n",
    "            for k in range (5):\n",
    "                act = code % 2\n",
    "                code = code // 2\n",
    "                state = k - 2\n",
    "                if act == 1:\n",
    "                    act = ACTION.high\n",
    "                else:\n",
    "                    act= ACTION.low\n",
    "                buf[state] = act\n",
    "            pll.append(buf)\n",
    "        return pll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_value 0 (7, 163.70370035420456) \n",
      "\n",
      "policy_value 1 (15, 183.52874459144365) \n",
      "\n",
      "policy_value 2 (3, 183.70369991591133) \n",
      "\n",
      "policy_value 3 (23, 188.22143392277948) \n",
      "\n",
      "policy_value 4 (6, 196.95448830072817) \n",
      "\n",
      "policy_value 5 (11, 197.86833568456407) \n",
      "\n",
      "policy_value 6 (19, 202.76093954187212) \n",
      "\n",
      "policy_value 7 (5, 205.4545426263376) \n",
      "\n",
      "policy_value 8 (31, 207.76698834554008) \n",
      "\n",
      "policy_value 9 (14, 210.99768186916225) \n",
      "\n",
      "policy_value 10 (13, 217.57575512135716) \n",
      "\n",
      "policy_value 11 (22, 217.5757551467629) \n",
      "\n",
      "policy_value 12 (27, 217.57575527790814) \n",
      "\n",
      "policy_value 13 (2, 222.21436701086424) \n",
      "\n",
      "policy_value 14 (21, 224.2424212317561) \n",
      "\n",
      "policy_value 15 (1, 230.3671043864758) \n",
      "\n",
      "policy_value 16 (10, 230.90908853824428) \n",
      "\n",
      "policy_value 17 (30, 232.0142014283098) \n",
      "\n",
      "policy_value 18 (29, 237.03911999800263) \n",
      "\n",
      "policy_value 19 (9, 237.5757546563585) \n",
      "\n",
      "policy_value 20 (18, 237.57575466212856) \n",
      "\n",
      "policy_value 21 (4, 237.57575523872) \n",
      "\n",
      "policy_value 22 (17, 244.15382797785006) \n",
      "\n",
      "policy_value 23 (12, 244.9852737668085) \n",
      "\n",
      "policy_value 24 (26, 247.3509263999301) \n",
      "\n",
      "policy_value 25 (25, 252.5714809699674) \n",
      "\n",
      "policy_value 26 (20, 253.15737803969617) \n",
      "\n",
      "policy_value 27 (28, 261.6528906559715) \n",
      "\n",
      "policy_value 28 (0, 266.21359008642565) \n",
      "\n",
      "policy_value 29 (8, 269.29918138571793) \n",
      "\n",
      "policy_value 30 (16, 277.16504273800206) \n",
      "\n",
      "policy_value 31 (24, 281.6528908619823) \n",
      "\n",
      "optimal policy \n",
      " defaultdict(<function Winner.cr_pl.<locals>.<lambda> at 0x7fe2e43400d0>, {-2: <ACTION.low: 'low'>, -1: <ACTION.low: 'low'>, 0: <ACTION.low: 'low'>, 1: <ACTION.high: 'high'>, 2: <ACTION.high: 'high'>}) \n",
      " value= 281.6528908619823\n"
     ]
    }
   ],
   "source": [
    "winner=Winner()\n",
    "policylist=winner.cr_pl()\n",
    "policy_b = defaultdict(lambda: np.zeros(0))\n",
    "for i in range (32):\n",
    "    V,policy = winner.P_E(policylist[i])\n",
    "    policy_b[i]=V[0]\n",
    "ordered_value=sorted(policy_b.items(),key=lambda x:x[1],reverse=False)\n",
    "for i in range(len(ordered_value)):\n",
    "    print('policy_value',format(i),ordered_value[i],'\\n')\n",
    "print('optimal policy \\n',policylist[ordered_value[31][0]],'\\n','value=',ordered_value[31][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
