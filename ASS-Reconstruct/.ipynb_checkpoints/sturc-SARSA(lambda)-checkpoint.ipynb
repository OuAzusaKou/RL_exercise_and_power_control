{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "from enum import Enum\n",
    "class ACTION(Enum):\n",
    "    high='high'\n",
    "    low='low' \n",
    "    \n",
    "class Winner:\n",
    "    def __init__(self):\n",
    "        self.energy_max = 10\n",
    "        self.energy_add = 2\n",
    "        self.energy_prob = 0.2\n",
    "        self.P_H = 0.55\n",
    "        self.P_L = 0.45\n",
    "        self.state = 0\n",
    "        self.done = 0\n",
    "        self.R = 0\n",
    "        self.highcost = 1\n",
    "        self.lowcost = -10\n",
    "        self.greedy = 0.1\n",
    "        self.energy=self.energy_max\n",
    "        self.discount=0.9\n",
    "        self.init_Q()\n",
    "        return\n",
    "    def reset_game(self):\n",
    "        self.state=0\n",
    "        self.done=0\n",
    "        self.energy=self.energy_max\n",
    "        return\n",
    "    def ba_policy(self,state,energy):\n",
    "        smp = np.random.uniform(0,1)\n",
    "        if smp > self.greedy:\n",
    "            if self.Q[state,energy,ACTION.high]>self.Q[state,energy,ACTION.low]:\n",
    "                ac=ACTION.high\n",
    "            else:\n",
    "                ac=ACTION.low\n",
    "        else:\n",
    "            sp = np.random.uniform(0,1)\n",
    "            if sp > 0.5:\n",
    "                ac=ACTION.high\n",
    "            else:\n",
    "                ac=ACTION.low\n",
    "        return ac\n",
    "        \n",
    "    #def ta_policy(self):\n",
    "    def sarsalam(self,numepisode,lam=0.9,stepsize=0.001):\n",
    "        self.init_Q()\n",
    "        for i_episode in range(1, numepisode+1):\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, numepisode), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            self.init_Etrace()\n",
    "            self.reset_game()\n",
    "            \n",
    "            action=self.ba_policy(state=self.state,energy=self.energy)\n",
    "            \n",
    "            while(1):\n",
    "                reward,nextstate,nextenergy,self.done=self.env_step(self.state,self.energy,action)\n",
    "                \n",
    "                nextaction = self.ba_policy(state=nextstate,energy=nextenergy)\n",
    "                \n",
    "                delta = reward + self.discount*self.Q[nextstate,nextenergy,nextaction] - self.Q[self.state,self.energy,action]\n",
    "                #print('delta',delta)\n",
    "                self.Etrace[self.state,self.energy,action] += 1\n",
    "                \n",
    "                #print(self.Etrace[self.state,action])\n",
    "                for state_t in range (7):\n",
    "                    for act_t in ACTION:\n",
    "                        for energy_t in range(1,self.energy_max+1):\n",
    "                            #print(self.Q[state_t,energy_t,act_t])\n",
    "                            #print('stepsize',stepsize,'delta',delta,'etr',self.Etrace[state_t,energy_t,act_t])\n",
    "                            self.Q[state_t,energy_t,act_t]+=stepsize*delta*self.Etrace[state_t,energy_t,act_t]\n",
    "                            \n",
    "                            self.Etrace[state_t,energy_t,act_t] = self.discount*lam*self.Etrace[state_t,energy_t,act_t]\n",
    "                self.state = nextstate\n",
    "                action = nextaction\n",
    "                self.energy=nextenergy\n",
    "                if self.done:\n",
    "                    break\n",
    "        return self.Q\n",
    "        \n",
    "        \n",
    "    def env_step(self,state,energy,action):\n",
    "        done=0\n",
    "        reward=0\n",
    "        sample = np.random.uniform(0, 1)\n",
    "        smp = np.random.uniform(0,1)\n",
    "\n",
    "        if action == 1:\n",
    "            if energy > 0 :\n",
    "                suc_prob = self.P_H\n",
    "                energy = energy - self.highcost\n",
    "            else:\n",
    "                suc_prob = self.P_L\n",
    "                reward = reward + self.lowcost\n",
    "        else: \n",
    "            suc_prob = self.P_L\n",
    "            reward = reward + self.lowcost\n",
    "        if sample < suc_prob:\n",
    "            state = state + 1\n",
    "        else:\n",
    "            state = state -1\n",
    "        if state >= 3 :\n",
    "            reward = reward + 1000\n",
    "            done = 1\n",
    "        if state <= (-3):\n",
    "            done = 1\n",
    "        if smp < self.energy_prob:\n",
    "            energy = energy + self.energy_add\n",
    "        if energy > self.energy_max:\n",
    "            energy = self.energy_max\n",
    "        #print(state)\n",
    "        return reward,state,energy,done\n",
    "    def init_Q(self):\n",
    "        self.Q = defaultdict(lambda:np.zeros(1))\n",
    "        for state in range(7):\n",
    "            for act in ACTION:\n",
    "                for energy in range(1,self.energy_max+1):\n",
    "                    self.Q[state-3,energy,act]=0.0\n",
    "        return\n",
    "    def init_Etrace(self):\n",
    "        self.Etrace=defaultdict(lambda:np.zeros(1))\n",
    "        for state in range(7):\n",
    "            for act in ACTION:\n",
    "                for energy in range(1,self.energy_max+1):\n",
    "                    self.Etrace[state-3,energy,act]=0\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "q = np.zeros(1)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ACTION.high\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "Q = defaultdict(lambda:np.zeros(1))\n",
    "class ACTION(Enum):\n",
    "    high='high'\n",
    "    low='low' \n",
    "for state in range(7):\n",
    "    for act in ACTION:\n",
    "        Q[state-3,act] = 1\n",
    "print(Q[-3,ACTION.low])\n",
    "print(ACTION.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'energy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f0abece46632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwinner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarsalam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-565a9b97d386>\u001b[0m in \u001b[0;36msarsalam\u001b[0;34m(self, numepisode, lam, stepsize)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnextstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnextenergy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnextaction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m#print('delta',delta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#print(self.Etrace[self.state,action])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'energy' is not defined"
     ]
    }
   ],
   "source": [
    "winner=Winner()\n",
    "\n",
    "Q=winner.sarsalam(500)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
