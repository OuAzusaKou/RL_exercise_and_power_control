{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "number = np.random.uniform(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-50, 1, 0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "ph = 0.55\n",
    "pl = 0.45\n",
    "rnd = 0\n",
    "flag = 0\n",
    "reward = 0\n",
    "ch = 50\n",
    "cl = 10\n",
    "def env(act,rnd):\n",
    "    flag=0\n",
    "    reward=0\n",
    "    sample = np.random.uniform(0, 1)\n",
    "    if act == 1:\n",
    "        threshold = ph\n",
    "        reward = reward - 50\n",
    "    else:\n",
    "        threshold = pl\n",
    "        reward = reward -10\n",
    "    if sample < threshold:\n",
    "        rnd = rnd + 1\n",
    "    else:\n",
    "        rnd = rnd -1\n",
    "    if rnd >= 3 :\n",
    "        reward = reward + 1000\n",
    "        flag = 1\n",
    "    if rnd <= (-3):\n",
    "        flag = 1\n",
    "        \n",
    "    return reward,rnd,flag\n",
    "env(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "MAX_ROUND = 3\n",
    "ACT_NUM = 2\n",
    "LR = 0.1\n",
    "class Player:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        return\n",
    "    def reset(self):\n",
    "        self.state_action_values = np.zeros((MAX_ROUND , ACT_NUM ))\n",
    "        self.greedy = 0.1\n",
    "        return\n",
    "    def behavior_policy(self,Q,state):\n",
    "        sample = np.random.uniform(0, 1)\n",
    "        if sample > self.greedy: \n",
    "            act = np.argmax(Q[state])\n",
    "        else:\n",
    "            act = np.random.randint(0, 2)\n",
    "        return act\n",
    "    def target_policy(self,state,Q):\n",
    "        value = Q[state]\n",
    "        return value\n",
    "    def Qlearning(self,num_episodes, max_time=100, discount_factor=1.0):\n",
    "        Q = defaultdict(lambda:np.zeros(2))\n",
    "        Q = self.init_Q(Q)\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            episode = []\n",
    "            state = 0\n",
    "            while(1):\n",
    "                action = self.behavior_policy(Q,state)\n",
    "                reward,next_state,flag = env(action,state)\n",
    "                episode.append((state, action, reward))\n",
    "                #print (next_state)\n",
    "                #print('argmax',np.argmax(Q[next_state]),'\\n')\n",
    "                Q[state][action] = Q[state][action] + LR*(reward + discount_factor*Q[next_state][np.argmax(Q[next_state])]-Q[state][action])\n",
    "                state = next_state\n",
    "                if flag == 1:\n",
    "                    break\n",
    "\n",
    "        print(Q)\n",
    "        #print(np.argmax(Q[state]))\n",
    "        return Q\n",
    "    def init_Q(self,Q):\n",
    "        for state in range(7):\n",
    "            state = state -3\n",
    "            Q[state]=[0.0,0.0]\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000.defaultdict(<function Player.Qlearning.<locals>.<lambda> at 0x7f898529fbf8>, {-3: [0.0, 0.0], -2: [66.08839608070397, 1.2404588483804853], -1: [213.11781862923584, 120.11404466011085], 0: [258.1748630030245, 341.3255833306732], 1: [525.6104230098875, 448.7977924365337], 2: [655.4490938313108, 763.5792086598523], 3: [0.0, 0.0]})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.Player.Qlearning.<locals>.<lambda>()>,\n",
       "            {-3: [0.0, 0.0],\n",
       "             -2: [66.08839608070397, 1.2404588483804853],\n",
       "             -1: [213.11781862923584, 120.11404466011085],\n",
       "             0: [258.1748630030245, 341.3255833306732],\n",
       "             1: [525.6104230098875, 448.7977924365337],\n",
       "             2: [655.4490938313108, 763.5792086598523],\n",
       "             3: [0.0, 0.0]})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player=Player()\n",
    "player.Qlearning(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
